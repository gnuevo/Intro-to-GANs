{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GaussianGAN2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gnuevo/Intro-to-GANs/blob/master/GaussianGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "TFetmD2ELkra",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# GANs -- Machine Learning Tokyo Workwhop\n",
        "\n",
        "__Mustafa Yagmur, Gregorio Nuevo Castro, Dimitris Katsios,  Suzana Ilic, Alisher Abdulkhaev\n",
        " -- June 30, 2018__\n",
        "\n",
        "_This tutorial is based on [this](https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f) code by [Dev Nag](https://medium.com/@devnag)._\n",
        "\n",
        "This notebook is available here [https://colab.research.google.com/drive/1KcY0MwxmadHKNhSRCgM6ZuUH8k0kM41-]()"
      ]
    },
    {
      "metadata": {
        "id": "VTJNBKBaL8wG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Welcome to this notebook. This is part of the material provided for the GAN Workwhop by _Machine Learning Tokyo_. In it we will explore some  ot the tools we will use during the workwhop:\n",
        "\n",
        "+ **Colab**, this Jupyter-notebook like environment (https://colab.research.google.com/)\n",
        "+ **PyTorch**, it is a deep learning framework with a flexible and frendly interface (https://pytorch.org/). We'll explain some aspects of PyTorch, but this is not an introduction/tutorial to the framework, so if you need extra information you can have a look at this tutorial https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html. Moreover, PyTorch is a pretty common framework nowadays, so if you have any question, Google it!\n",
        "\n",
        "Moreover, you'll do your first implementation of a very simple GAN. Yes! For real, a GAN! It may look frightening but believe me, a simple GAN is not **that** difficult to code. _Disclaimer, if you think that any GAN is easy to code you're very much wrong XD _"
      ]
    },
    {
      "metadata": {
        "id": "UNSJEBnsOFOi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Structure of this document\n",
        "\n",
        "This document is structured in several sections:\n",
        "\n",
        "+ Install PyTorch and import dependencies\n",
        "+ Explanation of the exercise\n",
        "+ Brief introduction to GANs\n",
        "+ Playing with noise and target distributions\n",
        "+ Explanation of the code\n",
        "    + D and G\n",
        "    + Training loop\n",
        "    + Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "LYIz91pqD6ji",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Install dependencies\n",
        "\n",
        "Before starting to code anything we have to install PyTorch in our VM in colab. It's not difficult, we can use `pip` inside colab, so just run the following cell and it'll install the framework. It'll take some minutes, be patient ;)"
      ]
    },
    {
      "metadata": {
        "id": "PKK_9lPzDu_g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h70cN0VuEiWW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import dependencies\n",
        "\n",
        "Now that we have PyTorch installed we can import **all** the dependencies that we're using throughout this tutorial\n",
        "\n",
        "+ `torch` and `torch.nn` are imports from the framework.\n",
        "+ `numpy` the almighty scientific computing package for python.\n",
        "+ `matplotlib` for visualisations.\n",
        "+ `google.colab.widgets` contains some utilities for colab itself, we're going to use it, for example, to structure the output in grids and stuff. It's not necessary that you understand this."
      ]
    },
    {
      "metadata": {
        "id": "C5mJtazjx8VQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# necesary imports\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import widgets\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib import animation, rc\n",
        "from IPython.display import HTML\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xvb8RAqkeOCs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hluc7y0HRNA7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So, are they imported? I hope so! "
      ]
    },
    {
      "metadata": {
        "id": "gB7KP73Pog0K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Proposed exercise\n",
        "\n",
        "In this case we're going to program a very simple GAN. We want to approximate a [Gaussian function](https://en.wikipedia.org/wiki/Gaussian_function) in one dimension. In the image below you can see some examples of Gaussians for different _means_ and _standard deviations_\n",
        "\n",
        "![Gaussian function](https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/1080px-Normal_Distribution_PDF.svg.png)\n",
        "\n",
        "Normally GANs use a Gaussian function as the `z` noise as input to the generator. In this case we're going to input Uniform noise. Why? Well, if the generator has to approximate a Gaussian function and we give it a Gaussian function as input to start with it seems a pretty easy problem, don't you think? By giving it an Uniform function, it has to find the way to apply some non-linear transformation to give it a Gaussian shape.\n"
      ]
    },
    {
      "metadata": {
        "id": "je6kP2n_obuO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Starting with the GAN\n",
        "\n",
        "If you've read the other material linked with this code, you should be already somehow familiar with the concept of GANs and how they work (more or less). As a summary, we need 2 networks that we call **Discriminator** and **Generator**. The **Generator** tries to generate samples like those in the dataset. For example, if the dataset only contains images of dogs and cats, then the generator will try to learn how to create dogs and cats. The **Discrimintar** tries to disginguish whether a certain sample is real or fake. Real means it came from the dataset, fake it means it was generated by the **Generator**. The **Discriminator** tries to detect the images coming from the **Generator**, whereas the **Generator** tries to fool the **Discriminator** and generate images it thinks are real images of cats and dogs.\n",
        "\n",
        "As we know, a GAN is defined by a series of elements:\n",
        "\n",
        "+ A **target distribution**, this is the probability distribution that we want to match. In our toy example this will be a Gaussian function. In a real scenario the propability distribution will be determined by the samples in our dataset (for example images of cats and dogs).\n",
        "+ The **noise sampler** (also called _prior_ in some papers), this is the input to our generator. The generator learns to convert noise samples into samples coming from the target distribution. In a real world example, this noise is usually sampled from a Gaussian distribution. In our case, however, we'll be sampling from an Uniform distribution. This distributions can have as many dimensions as we want. THIS IS AN IMPORTANT THING KEEP IT IN MIND.\n",
        "+ The **Discriminator** is a network that learns to distinguish real data (coming from our target distribution) from fake data (coming from the generator)\n",
        "+ The **Generator** is a network that learns to convert uncorrelated noise into samples resembling the probability distribution of our data or our target distribution.\n",
        "\n",
        "It may seem confusing what **Discriminator** or **Generator** really are. They do cool stuff that no other networks can do, is there anything special about them? It isn't! They are just regular neural networks. Concretely, the **Discriminator** is a binary classifier: is this sample real (1) or fake (0)? And the **Generator** is a network that takes a noise sample and outputs a sample with the shape of our target distribution. Both can be different types of networks: feed forward, convolutional, etc. In our case we use simple feed forward networks.\n",
        "They work because the example is simple, for mor complex problems we should find a better suited architecture.\n",
        "\n",
        "### Target distribution and input noise\n",
        "\n",
        "The first thing that we have to understand when coding any neural network is our data. So let's have a look at our target distribution and our noise distribution:\n",
        "\n",
        "+ **Target distribution** Gaussian function. You'll get to choose the _mean_ and _std_ ;)\n",
        "+ **noise distribution** Uniform samples between 0 and 1\n",
        "\n",
        "It's important to get familiar with what's the _shape_ of the data we're using."
      ]
    },
    {
      "metadata": {
        "id": "ePUS3Y89xk6e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Uniform sampler ###\n",
        "# Our noise sampler (Z) is a function that returns a vector of a desired length \n",
        "# with uniform samples between 0 and 1\n",
        "# we can use m and n to ask for a matrix of Uniform samples with the desired shape\n",
        "# therefore m is the dimension of the noise\n",
        "def uniform_sampler():\n",
        "  return lambda m, n: torch.rand(m, n)\n",
        "\n",
        "# we set the sampler to a variable so we can change it later if we want\n",
        "noise_sampler = uniform_sampler()\n",
        "# this is a 10x5 matrix with Uniform samples\n",
        "uniform_matrix = noise_sampler(10, 5)\n",
        "\n",
        "\n",
        "\n",
        "### Gaussian sampler ###\n",
        "# Our Gaussian sampler is a function that returns a vector of Gaussian samples\n",
        "# with a specific mean and std\n",
        "# We can use n to ask for a vector of shape 1xn\n",
        "def gaussian_sampler(mu, sigma):\n",
        "  return lambda n: torch.Tensor(np.random.normal(mu, sigma, (1, n)))\n",
        "\n",
        "# again, set it to a variable so we can change it later\n",
        "mu, sigma = 2, 1\n",
        "data_sampler = gaussian_sampler(mu, sigma)\n",
        "# this is a 1x5 vector with Gaussian samples\n",
        "gaussian_vector = data_sampler(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6joj0T5tz3sm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's have a look at the data. For that we can use histograms\n",
        "\n",
        "What you'll see next is a colab form. It allows you to change the values of the variables with a friendly user interface.\n",
        "\n",
        "+ **num_samples**, number of samples drawn from the distributions. The more samples you draw the more they'll look like what they are\n",
        "+ **gaussian_mean** and **gaussian_std**, parameters of the Gaussian\n",
        "+ **num_bins**, number of bars of the histogram\n",
        "+ **colors...**, well... give it a try ;)\n",
        "\n",
        "Please, experiment with different values of the number of samples, the parameters of the Gaussian, and the number of bins. Make sure you understand what's going on. You can't change the characteristics of the Uniform distribution because that our input noise, it's going to be fixed."
      ]
    },
    {
      "metadata": {
        "id": "-pYd0bc44Meg",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Define parameters\n",
        "num_samples = 8581 #@param {type:\"slider\", min:1, max:10000, step:10}\n",
        "gaussian_mean = -0.2 #@param {type:\"slider\", min:-3, max:3, step:0.2}\n",
        "gaussian_std = 2.1 #@param {type:\"slider\", min:0.1, max:3, step:0.1}\n",
        "\n",
        "num_bins = 91 #@param {type:\"slider\", min: 1, max:100, step:1}\n",
        "my_favourite_color_is = 'red' #@param [\"green\", \"blue\", \"red\", \"yellow\", \"black\", \"cyan\", \"magenta\"] {allow-input: false}\n",
        "but_I_also_like = 'blue' #@param [\"green\", \"blue\", \"red\", \"yellow\", \"black\", \"cyan\", \"magenta\"] {allow-input: false}\n",
        "\n",
        "# this creates a grid to make 2 plots side by side\n",
        "grid = widgets.Grid(1,2)\n",
        "\n",
        "# generate samples with the chosen characteristics\n",
        "noise = noise_sampler(1, num_samples)\n",
        "noise.view(num_samples).numpy()\n",
        "data_sampler = gaussian_sampler(gaussian_mean, gaussian_std)\n",
        "data = data_sampler(num_samples)\n",
        "\n",
        "# plot our data\n",
        "with grid.output_to(0, 0):\n",
        "  n, bins, patches = plt.hist(noise, num_bins, normed=1, facecolor=my_favourite_color_is,\n",
        "                                alpha=0.75)\n",
        "  ax = plt.gca()\n",
        "  ax.set_xlim(-0.25,1.25)\n",
        "  ax.set_ylim(0, 1.5)\n",
        "  plt.title(\"Noise distribution (Uniform between 0 and 1)\")\n",
        "  \n",
        "with grid.output_to(0, 1):\n",
        "  n, bins, patches = plt.hist(data, num_bins, normed=1, facecolor=but_I_also_like,\n",
        "                                alpha=0.75)\n",
        "  ax = plt.gca()\n",
        "  ax.set_xlim(-10,10)\n",
        "  ax.set_ylim(0, 0.7)\n",
        "  plt.title(\"Data distribution (Gaussian with mean={} and std={})\".format(gaussian_mean, gaussian_std))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sV1-07ZP9cAM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I hope now you understand a bit better what we want to do. We can use a GAN to approximate a different Gaussian every time.\n",
        "\n",
        "## Define the networks\n",
        "\n",
        "Now it's time to define the **Discriminator** and the **Generator**. As we said, they are simple feed forward networks\n",
        "\n",
        "### Discriminator\n",
        "\n",
        "The discriminator has to distinguish whether a sample is real (1) or fake (0). Does it sound familiar? It recalls what a binary classifier does, right? Indeed, we model the **Discriminator** as a binary classifier.\n",
        "\n",
        "Tip. Imagine for a moment that you're the **Discriminator**, and you have to distinguish whether a single sample comes from the real distribution or from the fake (generated) distribution. It seems like a difficult task, right? Do the following experiment. Use the above form and set `num_bins=10`. Now set `num_samples=1`. Execute the code and check what you see in the Gaussian plot on the right. Re run the code several times to see several samples. Do you see a Gaussian function? Clearly not. Maybe if you run enough times you'll understand that it is indeed a Gaussian function. But that's because you (as a human) have memory. Our **Discriminator** has not memory, so it can't remember past samples. Now increase `num_samples=100`. Re run the code several times again. Do you see a Gaussian now? Yes, you do, rigth? Maybe not a very good one (the more `num_samples` and `num_bins` the better is the Gaussian) but a Gaussian funciton indeed.\n",
        "\n",
        "A similar thing happens to the **Discriminator**, it's very difficult for it to distinguish if the samples are coming from the real or fake distributions if we give it samples from both distributions once at a time. To overcome this problem, what is usually done is to group a set of samples that come from the distributions and give them to the **Discriminator**. For example, we take 100 samples from the real data and we give them to the **Discriminator**, therefore it can have a look at the overall structure of the data. After that, we take another 100 samples from the fake distribution and we repeat the process. This is an usefull trick, we're goint to use it in our code: instead of feeding samples individually to the **Discriminator**, we're going to create groups of 100 samples in which all of them are real or all of them are fake.\n",
        "\n",
        "The Discriminator will look at a batch of samples and tell us if they look like comming from the target data distribution or if they look like fake data.\n",
        "\n",
        "Let's build a Discriminator with the following characteristics:\n",
        "\n",
        "+ It accepts an input of size `batch_size` (this is that _group_ of samples we talked about)\n",
        "+ It has 3 fully connected layers\n",
        "+ The activations are `elu`, except for the last layer, as it is a classifier, the last activation should be `sigmoid`"
      ]
    },
    {
      "metadata": {
        "id": "gsh8DkzB-kOc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def discriminator(input_size=100, hidden_size=50, output_size=1):\n",
        "  return nn.Sequential(\n",
        "      nn.Linear(input_size, hidden_size), nn.ELU(),   # this is the 1st layer\n",
        "      nn.Linear(hidden_size, hidden_size), nn.ELU(),  # this is the 2nd layer\n",
        "      nn.Linear(hidden_size, 1), nn.Sigmoid()         # this is the 3rd layer\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GQCMQjSgBx2u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generator\n",
        "\n",
        "On the other side, the Generator just takes a vector of random noise as input and outputs a single sample. We can code a Generator similar to our Discriminator. So, our Generator\n",
        "\n",
        "+ Accepts an input of size `batch_size` (this is that _group_ of samples we talked about). Each of those samples comes from the noise Uniform distribution and can have n dimensions. So, for example, we can create a set of `batch_size` noise samples, each of one has dimensionality 1, 2, 3, 10, etc.\n",
        "+ It has 3 fully connected layers\n",
        "+ The activations are `elu` in the middle layers\n",
        "+ The output layer has no activation. Why? Well, because we want our output to be non restricted. We have to model a Gaussian function, that is defined from `-infinite` to `+infinite`. If we apply an activation function we're restricting that output. For example, `sigmoid` only outputs `[0,1]`, `tanh` outputs `[-1,1]`, `ReLU` outptus `[0, +infinite]`, and so on."
      ]
    },
    {
      "metadata": {
        "id": "YsCvZ_SJFgI8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generator(input_size=1, hidden_size=50, output_size=1):\n",
        "  return nn.Sequential(\n",
        "      nn.Linear(input_size, hidden_size), nn.ELU(),    # this is the 1st layer\n",
        "      nn.Linear(hidden_size, hidden_size), nn.ELU(),   # this is the 2nd layer\n",
        "      nn.Linear(hidden_size, output_size)              # this is the 3rd layer\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CrWlz5qVFwRy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "So we have all the elements defined. Now here it comes the complicated part. The training loop is probably one of the more complicated things to code and certanly what makes a GAN to be a GAN. After instantiating the necessary variables we're going to\n",
        "\n",
        "+ Train the Discriminator to distinguish samples from the real and fake distributions. Using one group or batch from each of those distributions at a time.\n",
        "+ Train the Generator to generate samples to get realistic images every time\n",
        "\n",
        "### Explanation of the code\n",
        "\n",
        "When you see the code for the `main()` function you'll probably get crazy, throw your computer through the window, run away and never look back to that GAN stuff that you tried to learn once. Because of that, I'll try to explain here what the code is doing. Pay attentioin and that will help you to understand it much faster.\n",
        "\n",
        "The code is structured in several parts. On the first level, there's the (a) initialisation of variables and parameters, and (b) the trainin loop.\n",
        "\n",
        "#### Initialisation\n",
        "\n",
        "We instantiate\n",
        "\n",
        "+ Our network: Discriminator (D) and Generator (G)\n",
        "+ The optimisers: they are in charge of uptading the weights of G and D, and therefore getting better over time\n",
        "+ The loss function: in this case we use binary crossentropy (because it is a classification task: real/fake)\n",
        "+ Our samplers, both from the target and noise distribution\n",
        "\n",
        "#### Training loop\n",
        "\n",
        "##### Train D\n",
        "\n",
        "We first train D with some real and fake samples\n",
        "\n",
        "With real samples\n",
        "+ Zero previous gradients with `zero_grad()`\n",
        "+ Get some **real samples** (`batch_size` of them)\n",
        "+ Compute the score the discriminator gives to them\n",
        "+ Compute the loss. In this case all the samples are **real**. So we want the discriminator to output 1 (this is our target), so we use a vector containing a single 1 with `torch.ones_like()`.\n",
        "+ Compute the loss gradients with `backward()`\n",
        "+ Optimise the parameters\n",
        "\n",
        "With fake samples we have a very similar process\n",
        "+ Zero previous gradients with `zero_grad()`\n",
        "+ Get some **fake samples** using the generator and the noise sampler for that (`batch_size` of them)\n",
        "+ Compute the score the discriminator gives to them\n",
        "+ Compute the loss. In this case all the samples are **fake**. So we want the discriminator to output 0 (this is our target), so we use a vector containing a single  with `torch.zeros_like()`.\n",
        "+ Compute the loss gradients with `backward()`\n",
        "+ Optimise the parameters\n",
        "\n",
        "##### Train G\n",
        "\n",
        "We generate some random samples using G\n",
        "\n",
        "+ We create `batch_size` samples with G like we've been done before\n",
        "+ We score those samples using D\n",
        "+ Now we have to compute a loss like above. But what is our target? 0 or 1? They are generated samples, therefore the target should we 0. But we would like the Discriminator to tell us that those are real samples (1), that would mean that the Generator is doing great. Indeed, that's what we're going to do. We're going to say that our target is 1, we pretend these are real samples. And actualise the weights of G accordingly. This means, our loss and gradients are answering the question\n",
        "\n",
        "```\n",
        "How do the weights of G should change so the generated samples will be considered a bit closer to a real sample (1) by D?.\n",
        "```\n",
        "\n",
        "+ Compute the loss gradients with `backward()`\n",
        "+ Optimise the parameters\n",
        "\n",
        "Notice that D and G are never optimised at a time. If we train D with some samples, we never train G on the same samples, and vice versa.\n",
        "\n",
        "### Hyperparameters\n",
        "\n",
        "Like before, we're going to set some hyperparameters in a colab form so we can change them easily\n",
        "\n",
        "+ **iterations**, for how long are we training the GAN?\n",
        "+ **mu** and **sigma**, the parameters of the Gaussian\n",
        "+ **input_size**, dimensionality  of the input noise.\n",
        "\n",
        "**input_size** is a very important parameter, we'll see later. Please, for now leave these values as default and continue. You'll have time to hack them later ;)\n",
        "\n",
        "Also notice there are other parameters that are not included in the form. So you have to change them in the code. Leave them as they are for now as well.\n",
        "\n",
        "```python\n",
        "# print some results every 200 steps\n",
        "print_step = 200\n",
        "\n",
        "# these are some hyperparameters to for the network architecture\n",
        "hidden_size = 50\n",
        "output_size = 1\n",
        "learning_rate = 2e-4\n",
        "batch_size = 100\n",
        "test_num = 10000 # number of samples used for testing\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "XUsdWbujIli4",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Define hyperparameters\n",
        "iterations = 19000 #@param {type:\"slider\", min:1000, max:50000, step:2000}\n",
        "mu = 1.5 #@param {type:\"slider\", min:-5, max:5, step:0.1}\n",
        "sigma = 1 #@param {type:\"slider\", min:0.1, max:3, step:0.1}\n",
        "input_size = 1 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "# print some results every 200 steps\n",
        "print_step = 200\n",
        "\n",
        "# these are some hyperparameters to for the network architecture\n",
        "hidden_size = 50\n",
        "output_size = 1\n",
        "\n",
        "learning_rate = 2e-4\n",
        "batch_size = 100\n",
        "test_num = 10000 # number of samples used for testing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JxN9qsolp6bL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Utility functions\n",
        "\n",
        "Here we're going to define some utility functions. They are useful for visualisation purposes but they are not vital for the GAN to run. So you can give it a look, but don't worry if you don't fully understand them. The important is the `train()` function below."
      ]
    },
    {
      "metadata": {
        "id": "df5ICVhmKtzs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# utility function to plot the histograms of training\n",
        "def make_histogram(data_real, data_fake, i, n_bins=100):\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    axes = plt.subplot(111)\n",
        "    n, bins, patches = plt.hist(data_real, n_bins, normed=1, facecolor='green',\n",
        "                                alpha=0.75, label=\"Real data\")\n",
        "    n, bins, patches = plt.hist(data_fake, bins, normed=1, facecolor='blue',\n",
        "                                alpha=0.75, label=\"Generated data\")\n",
        "    axes.set_xlim(-5, 8)\n",
        "    axes.set_ylim(0, 1)\n",
        "    plt.title(\"Probability distribution of real and fake data\")\n",
        "    plt.legend([\"Real data\", \"Generated data\"])\n",
        "    plt.tight_layout()\n",
        "\n",
        "# utility function to plot the evolution of the errors during training\n",
        "def plot_errors(real_loss_buffer, fake_loss_buffer, g_loss_buffer):\n",
        "    fig = plt.figure(0, figsize=(10, 10))\n",
        "    plt.subplot(311)\n",
        "    plt.plot(real_loss_buffer)\n",
        "    plt.title(\"D real loss\")\n",
        "\n",
        "    plt.subplot(312)\n",
        "    plt.plot(fake_loss_buffer)\n",
        "    plt.title(\"D fake loss\")\n",
        "\n",
        "    plt.subplot(313)\n",
        "    plt.plot(g_loss_buffer)\n",
        "    plt.title(\"G fake loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ph0ccsE2seAz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Actual training function"
      ]
    },
    {
      "metadata": {
        "id": "Gxr0YTX-Ip2Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "train_gaussian = None\n",
        "train_generation = None\n",
        "train_errors = None\n",
        "count = 0\n",
        "def train():\n",
        "    # ==========================================================================\n",
        "    # INITIALISATION\n",
        "    # ==========================================================================\n",
        "    \n",
        "    # this variable is for display purposes, you can ignore it!\n",
        "    grid = widgets.Grid(2,2)\n",
        "    global train_gaussian, train_generation, train_errors\n",
        "    train_generation = []\n",
        "    train_errors = []\n",
        "  \n",
        "    # instantiate Generator and Discriminator\n",
        "    G = generator(input_size, hidden_size, output_size)\n",
        "    D = discriminator(batch_size, hidden_size, output_size)\n",
        "    # set the optimisers\n",
        "    g_optimizer = torch.optim.Adam(G.parameters(), lr=learning_rate)\n",
        "    d_optimizer = torch.optim.Adam(D.parameters(), lr=learning_rate)\n",
        "\n",
        "    # loss function: Binary CrossEntropy\n",
        "    loss_function = nn.BCELoss()\n",
        "\n",
        "    # get our data and noise samplers\n",
        "    data_sampler = gaussian_sampler(mu, sigma)\n",
        "    test_data = data_sampler(test_num)\n",
        "    train_gaussian = test_data\n",
        "\n",
        "    # this list will keep track of the errors during training\n",
        "    real_loss_buffer = []\n",
        "    fake_loss_buffer = []\n",
        "    g_loss_buffer = []\n",
        "    \n",
        "    global count\n",
        "    count = 0\n",
        "    # ==========================================================================\n",
        "    # TRAINING LOOP\n",
        "    # ==========================================================================\n",
        "    for i in range(iterations):\n",
        "        # ========================================\n",
        "        # TRAIN D\n",
        "        # ========================================\n",
        "        \"\"\"TRAINING D\n",
        "            Remember, first we train D on real samples\n",
        "            + delete previous gradients\n",
        "            + get real samples\n",
        "            + score them with D\n",
        "            + compute the loss with target=1\n",
        "            + compute gradients and optimise\n",
        "        \"\"\"\n",
        "        # zero the gradients\n",
        "        D.zero_grad()\n",
        "\n",
        "        # take real samples\n",
        "        real_data = data_sampler(batch_size)\n",
        "        # pass the real samples through the discriminator\n",
        "        real_score = D(real_data)\n",
        "        # derive loss on real samples\n",
        "        real_loss = loss_function(real_score, torch.ones_like(real_score))\n",
        "        # save loss for visualisation purposes\n",
        "        real_loss_buffer.append(real_loss)\n",
        "\n",
        "        # compute gradients and optimize\n",
        "        real_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        \"\"\"TRAINING D\n",
        "            Now we train D with fake samples\n",
        "            + delete previous gradients\n",
        "            + get fake samples using G\n",
        "            + score them with D\n",
        "            + compute the loss with target=0\n",
        "            + compute gradients and optimise\n",
        "        \"\"\"\n",
        "        D.zero_grad()\n",
        "\n",
        "        # feed fake images\n",
        "        noise_data = noise_sampler(batch_size, input_size)\n",
        "        fake_data = G(noise_data)\n",
        "\n",
        "        #compute scores(logits)\n",
        "        fake_score = D(fake_data.t())\n",
        "\n",
        "        #derive loss on fake samples\n",
        "        fake_loss = loss_function(fake_score, torch.zeros_like(fake_score))\n",
        "        # save loss for visualisation purposes\n",
        "        fake_loss_buffer.append(fake_loss)\n",
        "\n",
        "        # compute gradients and optimize\n",
        "        fake_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        # ========================================\n",
        "        # TRAIN G\n",
        "        # ========================================\n",
        "        \"\"\"TRAINING G\n",
        "            + delete previous gradients\n",
        "            + get fake samples using G\n",
        "            + score them with D\n",
        "            + compute the loss with target=1\n",
        "            + compute gradients and optimise\n",
        "        \"\"\"\n",
        "        G.zero_grad()\n",
        "\n",
        "        # prepare fake images through Generator\n",
        "        noise_data = noise_sampler(batch_size, input_size)\n",
        "        fake_data = G(noise_data)\n",
        "        \n",
        "        # compute scores(logits)\n",
        "        g_score = D(fake_data.t())\n",
        "        \n",
        "        #derive loss on fake samples\n",
        "        g_loss = loss_function(g_score, torch.ones_like(g_score))\n",
        "        # save loss for visualisation purposes\n",
        "        g_loss_buffer.append(g_loss)\n",
        "        \n",
        "        # compute gradients and optimize\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        \n",
        "        # ========================================\n",
        "        # VISUALISE TRAINING\n",
        "        # (you don't have to understand this)\n",
        "        # ========================================\n",
        "        # print training statistics every print_step\n",
        "        if (i + 1) % print_step == 0:\n",
        "            # print current errors\n",
        "            with grid.output_to(1,0):\n",
        "                if (i + 1) % (15 * print_step) == 0:\n",
        "                    grid.clear_cell()\n",
        "                print(\"{:>6} -> D_real_loss {:.6f}  D_fake_loss {:.6f}  \"\n",
        "                      \"G_fake_loss {:.6f}\".format(\n",
        "                    i+1,\n",
        "                    real_loss_buffer[-1],\n",
        "                    fake_loss_buffer[-1],\n",
        "                    g_loss_buffer[-1]\n",
        "                ))\n",
        "                \n",
        "            # plot distributions\n",
        "            fake_data = torch.Tensor([])\n",
        "            for _ in range(test_num // batch_size):\n",
        "                noise_data = noise_sampler(batch_size, input_size)\n",
        "                fake_data = torch.cat((fake_data, G(noise_data).t()), 1)\n",
        "            fake_data = fake_data[:, :test_num]\n",
        "            train_generation.append(fake_data)\n",
        "            with grid.output_to(0, 0):\n",
        "                grid.clear_cell()\n",
        "                make_histogram(test_data.data[0,:].numpy(), fake_data.data[\n",
        "                    0,:].numpy(), i)\n",
        "                plt.savefig(\"figure_{}.png\".format(count))\n",
        "                count += 1\n",
        "                \n",
        "            # plot training errors\n",
        "            with grid.output_to(0, 1):\n",
        "                grid.clear_cell()\n",
        "                plot_errors(real_loss_buffer, fake_loss_buffer, g_loss_buffer)\n",
        "                \n",
        "    train_errors = [real_loss_buffer, fake_loss_buffer, g_loss_buffer]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KJ2C9iPmstS2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Run it\n",
        "\n",
        "Now that everything is defined, it's time to run it.\n",
        "\n",
        "_Note: it's not going to work very well at the first run. Don't let it affect you, we'll improve that later_"
      ]
    },
    {
      "metadata": {
        "id": "FqX3F17nI91-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# fingers crossed\n",
        "train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ixZg73mNxZ2e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Analysis of the results\n",
        "\n",
        "So wow! What's that?! Are you kidding me!? All this effort (_effort in a figurative way, you could have gotten to here just by pressing Shift+Enter_ :|)  to get such a shitty distribution (blue)?\n",
        "\n",
        "Yes! Nobody said this was easy. GANs are not easy stuff to train. Certainly we can make some improvements.\n",
        "\n",
        "But before going further, did you understand what's going on?\n",
        "\n",
        "+ Plot on the left shows the probability distribution of both real and fake data during training. So you can see how the Generator is training. Do you see that is usually always changing? And sometimes looks like it's getting stable and suddenly it changes a lot? Well, that's one of the issues with GANs, **stability** is a problem.\n",
        "\n",
        "+ Plot on the right shows the evolution of the errors, from top to bottom\n",
        "  + **D real loss**, the error of D when classifying real samples, the lower it is the better **real** samples are classified as real\n",
        "  + **D fake loss**, the error of D when classifying fake samples, the lower it is the better **fake** samples are classified as real\n",
        "  + **G fake loss**, the error G samples, the lower it is the better is G fooling D.\n",
        "\n",
        "  But what happend here? What a crazy plot, right?! Indeed, it's difficult to understand the meaning of the error plots of GANs. If you're familiar with neural networks you've probably seen many accuracy or error plots. In which the error starts very high and decreases over time, or the accuracy starts very low and increases over time. Well, here is not like that. Why, because the error of D depends on G (if G gets better then the error will tend to raise); but at the same time the error of G depends on D (if D gets better, it is going to catch G easier and therefore output a greater error). But as both D and G improve over time, there is no baseline we can use to compute an objective accuracy. That's why normally the performance of the GANs has to be assessed manually. Definitive conclusions can't be extracted from the error measures."
      ]
    },
    {
      "metadata": {
        "id": "_l7cCjjE0WyH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Let's fix this\n",
        "\n",
        "Ok, I said it wasn't going to be good at the beginning. But I also said we could fix it. And we indeed can. The key is in the hyperparameters. Do you remember that I wrote `THIS IS AN IMPORTANT THING KEEP IT IN MIND` somewhere above. Well, if I did it's probably because `THAT IS AN IMPORTANT THING KEEP IT IN MIND`.\n",
        "\n",
        "We'll discuss this further during the workshop, but the noise Z creates a latent space of variables. The GAN just learns to give meaning or structure to that latent space. Therefore, its dimension is important. Higher dimensions, more expressivity that the GAN can have. It'll learn to somehow correlate the dimensions in Z to give it some meaning when generating a sample.\n",
        "\n",
        "So what I want you to know is to try and change the dimensionality of Z. The variable that does this is `input_size`. Start by doing `input_size=2` and re-run the training code. \n",
        "\n",
        "Don't worry, I've copyed the _form_ for you again here. So you just have to select the new values for the variables and run the cell. It'll make the GAN to train with the new configuration."
      ]
    },
    {
      "metadata": {
        "id": "qi4zAnPkJHTq",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Define hyperparameters\n",
        "iterations = 45000 #@param {type:\"slider\", min:1000, max:50000, step:2000}\n",
        "mu = -0.3 #@param {type:\"slider\", min:-5, max:5, step:0.1}\n",
        "sigma = 0.6 #@param {type:\"slider\", min:0.1, max:3, step:0.1}\n",
        "input_size = 3 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "# print some results every 200 steps\n",
        "print_step = 200\n",
        "\n",
        "# these are some hyperparameters to for the network architecture\n",
        "hidden_size = 50\n",
        "output_size = 1\n",
        "\n",
        "learning_rate = 2e-4\n",
        "batch_size = 100\n",
        "test_num = 10000 # number of samples used for testing\n",
        "\n",
        "\n",
        "# better luck this time\n",
        "train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yHtHcfhb4g3M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Better results\n",
        "\n",
        "Wow! Things improved, didn't they? Mmmm... I wonder what would happen if we use even more input dimensions. If only there was a way of figuring it out... I guess we'll never now! :P"
      ]
    },
    {
      "metadata": {
        "id": "BQTwt9Po3Cn-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Further things to try\n",
        "\n",
        "By now you should have  a general idea of how things work. But how do all the rest of the hyperparameters affect the training. I don't know. I don't even know to which extent is possible to predic its effect. So let's try it, shall we?\n",
        "\n",
        "Of course, we also encourage you to change `mu` and `sigma` of the Gaussian to fit different target distributions.\n",
        "\n",
        "Appart from the variables in the form, there are other ones you can also change, for example.\n",
        "\n",
        "```python\n",
        "hidden_size = 50\n",
        "learning_rate = 2e-4\n",
        "batch_size = 100\n",
        "```\n",
        "\n",
        "Have fun, see all of you in the workshop :)"
      ]
    },
    {
      "metadata": {
        "id": "iG6Nd5uOUOGW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Do a funny video of the training\n",
        "\n",
        "Now we're finally going to do a video with all the training information. During the execution of the program wi silently stored all the plots in disk as images. Their names are `figure_0.png`, `figure_1.png` and so on. Check that the files are actually there with `ls`"
      ]
    },
    {
      "metadata": {
        "id": "koSFQfPdNe_U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!ls | wc -l\n",
        "print(\"Number of images saved\", count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dvAlMvcVtLBI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If the previous command fails you may need to restart the runtime. Do it under the `Runtime` menu on the menu bar on the top.\n",
        "\n",
        "**Note: In case the runtime is restarted, we must import all the modules again. It is also needed to the variable `count` to its right value (the number of images saved during training). Luckly, the following script should fix this. Just click the checkbox and run.**"
      ]
    },
    {
      "metadata": {
        "id": "wJ0me-HptOSK",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Did you have to restart? (check and run if you did)\n",
        "restart_checkbox = False #@param {type:\"boolean\"}\n",
        "\n",
        "if restart_checkbox:\n",
        "  # import again\n",
        "  print(\"Importing modules\")\n",
        "  import torch\n",
        "  from torch import nn\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "  from google.colab import widgets\n",
        "  import matplotlib.gridspec as gridspec\n",
        "  from matplotlib import animation, rc\n",
        "  from IPython.display import HTML\n",
        "  from PIL import Image\n",
        "  \n",
        "  # read files\n",
        "  print(\"Reading the number of images\")\n",
        "  from os import listdir\n",
        "  count = len(list(filter(lambda name: \"figure_\" in name, listdir())))\n",
        "  print(\"Number of images saved\", count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FSb5A5omXJxr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following cell defines the logic that takes those images and creates the animation."
      ]
    },
    {
      "metadata": {
        "id": "ge8cKj07JrNj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AnimObject(object):\n",
        "    def __init__(self, images):\n",
        "        print(len(images))\n",
        "        self.fig, self.ax = plt.subplots()\n",
        "        self.ax.set_title(\"\")\n",
        "        self.fig.set_size_inches((8, 8))\n",
        "        self.plot = plt.imshow(images[0])\n",
        "        plt.tight_layout()\n",
        "        self.images = images\n",
        "        \n",
        "    def init(self):\n",
        "        self.plot.set_data(self.images[0])\n",
        "        self.ax.grid(False)\n",
        "        return (self.plot,)\n",
        "      \n",
        "    def animate(self, i):\n",
        "        self.plot.set_data(self.images[i])\n",
        "        self.ax.grid(False)\n",
        "        self.ax.set_xticks([])\n",
        "        self.ax.set_yticks([])\n",
        "        self.ax.set_title(\"index {}\".format(i))\n",
        "        return (self.plot,)\n",
        "\n",
        "def get_figures(template, indices):\n",
        "    import os.path\n",
        "    images = []\n",
        "    for index in indices:\n",
        "        if os.path.isfile(template.format(index)):\n",
        "            images.append(Image.open(template.format(index)))\n",
        "    return images\n",
        "  \n",
        "images = get_figures(\"figure_{}.png\", range(count))\n",
        "animobject = AnimObject(images)\n",
        "anim = animation.FuncAnimation(\n",
        "              animobject.fig,\n",
        "              animobject.animate,\n",
        "              frames=len(animobject.images),\n",
        "              interval=150,\n",
        "              blit=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uITJgA-dXRUj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualise the animation\n",
        "\n",
        "Finally we can just convert the animation to html so we can watch it in the browser."
      ]
    },
    {
      "metadata": {
        "id": "5V_-9hBLSEKe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "HTML(anim.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ITaYSTObD6K5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}